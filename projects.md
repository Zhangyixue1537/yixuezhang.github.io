---
layout: page
title: Projects
tagline: A list of my projects and publications
permalink: /projects.html
ref: projects
order: 0
---

## Acessible Taxi


<table border="0" cellspacing="200" cellpadding="2"><tbody>
<tr>
<td valign="top">
    <a href="https://arxiv.org/abs/2102.06559">
            <img src="./figures/yuyu.jpg" alt="Link" width="250" height="250">
    </a>
</td>
<td valign="top">
    <b>Infinitely Deep Bayesian Neural Networks with Stochastic Differential Equations</b>
    <p>
We perform scalable approximate inference in a recently-proposed family of continuous-depth Bayesian neural networks. In this model class, uncertainty about separate weights in each layer produces dynamics that follow a stochastic differential equation (SDE). We demonstrate gradient-based stochastic variational inference in this infinite-parameter setting, producing arbitrarily-flexible approximate posteriors. We also derive a novel gradient estimator that approaches zero variance as the approximate posterior approaches the true posterior. This approach inherits the memory-efficient training and tunable precision of neural ODEs.
    </p>
    <a href="https://winniexu.ca/">Winnie Xu</a>,
    <a href="http://www.cs.toronto.edu/~rtqichen/">Ricky Tian Qi Chen</a>,
    <a href="http://lxuechen.com/">Xuechen Li</a>,
    <a href="https://www.cs.toronto.edu/~duvenaud/">David Duvenaud</a>
    <br>
    <em>Artificial Intelligence and Statistics</em>, 2022.
    <br>
    <a href="https://arxiv.org/abs/2102.06559">paper</a>
    | <a href="https://github.com/xwinxu/bayeSDE">code</a>
    | <a href="talks/sdebnn.pdf">slides</a>
    | <a href="papers/sdebnn.bib">bibtex</a>
</td>
</tr>
    
<tr><td height="10px"></td></tr>  
    
</tbody></table>

<img align="left" width="250" height="250" src="./figures/yuyu.jpg">
We perform scalable approximate inference in a recently-proposed family of continuous-depth Bayesian neural networks. In this model class, uncertainty about separate weights in each layer produces dynamics that follow a stochastic differential equation (SDE). We demonstrate gradient-based stochastic variational inference in this infinite-parameter setting, producing arbitrarily-flexible approximate posteriors. We also derive a novel gradient estimator that approaches zero variance as the approximate posterior approaches the true posterior. This approach inherits the memory-efficient training and tunable precision of neural ODEs.

[**Yixue Zhang**](https://zhangyixue1537.github.io),  [Steve Farber](https://github.com/jekyll/jekyll).\
_Transportation Journal, 2021_\
[paper](https://github.com/jekyll/jekyll) | [slides](https://github.com/jekyll/jekyll)

___

<img align="left" width="250" height="250" src="./figures/yuyu.jpg">
We perform scalable approximate inference in a recently-proposed family of continuous-depth Bayesian neural networks. In this model class, uncertainty about separate weights in each layer produces dynamics that follow a stochastic differential equation (SDE). We demonstrate gradient-based stochastic variational inference in this infinite-parameter setting, producing arbitrarily-flexible approximate posteriors. We also derive a novel gradient estimator that approaches zero variance as the approximate posterior approaches the true posterior. This approach inherits the memory-efficient training and tunable precision of neural ODEs.

[**Yixue Zhang**](https://zhangyixue1537.github.io),  [Steve Farber](https://github.com/jekyll/jekyll).\
_Transportation Journal, 2021_\
[paper](https://github.com/jekyll/jekyll) | [slides](https://github.com/jekyll/jekyll)


___

## Belleville Bus on Demand 
